{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collaboration and Competition\n",
    "\n",
    "---\n",
    "\n",
    "### 1. Start the Environment\n",
    "\n",
    "Run the next code cell to install a few packages.  This line will take a few minutes to run!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mtensorflow 1.7.1 has requirement numpy>=1.13.3, but you'll have numpy 1.12.1 which is incompatible.\u001b[0m\r\n",
      "\u001b[31mipython 6.5.0 has requirement prompt-toolkit<2.0.0,>=1.0.15, but you'll have prompt-toolkit 2.0.10 which is incompatible.\u001b[0m\r\n"
     ]
    }
   ],
   "source": [
    "!pip -q install ./python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The environment can be accessed at the file path provided below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\t\n",
      "Unity brain name: TennisBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 8\n",
      "        Number of stacked Vector Observation: 3\n",
      "        Vector Action space type: continuous\n",
      "        Vector Action space size (per agent): 2\n",
      "        Vector Action descriptions: , \n"
     ]
    }
   ],
   "source": [
    "from unityagents import UnityEnvironment\n",
    "import numpy as np\n",
    "\n",
    "env = UnityEnvironment(file_name=\"/data/Tennis_Linux_NoVis/Tennis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Environments contain **_brains_** which are responsible for deciding the actions of their associated agents. Here we check for the first brain available, and set it as the default brain we will be controlling from Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the default brain\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Examine the State and Action Spaces\n",
    "\n",
    "Run the code cell below to print some information about the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of agents: 2\n",
      "Size of each action: 2\n",
      "There are 2 agents. Each observes a state with length: 24\n",
      "The state for the first agent looks like: [ 0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.         -6.65278625 -1.5        -0.          0.\n",
      "  6.83172083  6.         -0.          0.        ]\n"
     ]
    }
   ],
   "source": [
    "# reset the environment\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "\n",
    "# number of agents \n",
    "num_agents = len(env_info.agents)\n",
    "print('Number of agents:', num_agents)\n",
    "\n",
    "# size of each action\n",
    "action_size = brain.vector_action_space_size\n",
    "print('Size of each action:', action_size)\n",
    "\n",
    "# examine the state space \n",
    "states = env_info.vector_observations\n",
    "state_size = states.shape[1]\n",
    "print('There are {} agents. Each observes a state with length: {}'.format(states.shape[0], state_size))\n",
    "print('The state for the first agent looks like:', states[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Tennis](https://github.com/Unity-Technologies/ml-agents/blob/master/docs/images/tennis.png?raw=true)\n",
    "\n",
    "* Set-up: Two-player game where agents control rackets to bounce ball over a\n",
    "  net.\n",
    "* Goal: The agents must bounce ball between one another while not dropping or\n",
    "  sending ball out of bounds.\n",
    "* Agents: The environment contains two agent linked to a single Brain.\n",
    "* Agent Reward Function (independent):\n",
    "  * +0.1 To agent when hitting ball over net.\n",
    "  * -0.01 To agent who let ball hit their ground, or hit ball out of bounds.\n",
    "* Brains: One Brain with the following observation/action space.\n",
    "  * Vector Observation space: 8 variables corresponding to position and velocity\n",
    "    of ball and racket.\n",
    "  * Vector Action space: (Continuous) Size of 2, corresponding to movement\n",
    "    toward net or away from net, and jumping.\n",
    "* Reset Parameters: Three\n",
    "    * angle: Angle of the racket from the vertical (Y) axis.\n",
    "      * Default: 55\n",
    "      * Recommended Minimum: 35 \n",
    "      * Recommended Maximum: 65\n",
    "    * gravity: Magnitude of gravity\n",
    "      * Default: 9.81\n",
    "      * Recommended Minimum: 6\n",
    "      * Recommended Maximum: 20\n",
    "    * scale: Specifies the scale of the ball in the 3 dimensions (equal across the three dimensions)\n",
    "      * Default: 1\n",
    "      * Recommended Minimum: 0.2\n",
    "      * Recommended Maximum: 5\n",
    "* Benchmark Mean Reward: 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Take Random Actions in the Environment\n",
    "\n",
    "In the next code cell, you will learn how to use the Python API to control the agent and receive feedback from the environment.\n",
    "\n",
    "**Note:** you should set `train_mode=True` to restart the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total score (averaged over agents) this episode: 0.04500000085681677\n",
      "Total score (averaged over agents) this episode: -0.004999999888241291\n",
      "Total score (averaged over agents) this episode: -0.004999999888241291\n",
      "Total score (averaged over agents) this episode: -0.004999999888241291\n",
      "Total score (averaged over agents) this episode: -0.004999999888241291\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):                                         # play game for 5 episodes\n",
    "    env_info = env.reset(train_mode=False)[brain_name]     # reset the environment    \n",
    "    states = env_info.vector_observations                  # get the current state (for each agent)\n",
    "    scores = np.zeros(num_agents)                          # initialize the score (for each agent)\n",
    "    while True:\n",
    "        actions = np.random.randn(num_agents, action_size) # select an action (for each agent)\n",
    "        actions = np.clip(actions, -1, 1)                  # all actions between -1 and 1\n",
    "        env_info = env.step(actions)[brain_name]           # send all actions to tne environment\n",
    "        next_states = env_info.vector_observations         # get next state (for each agent)\n",
    "        rewards = env_info.rewards                         # get reward (for each agent)\n",
    "        dones = env_info.local_done                        # see if episode finished\n",
    "        scores += env_info.rewards                         # update the score (for each agent)\n",
    "        states = next_states                               # roll over states to next time step\n",
    "        if np.any(dones):                                  # exit loop if episode finished\n",
    "            break\n",
    "    print('Total score (averaged over agents) this episode: {}'.format(np.mean(scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Train the agents\n",
    "\n",
    "The Multi-Agent Deep Deterministic Policy Gradient(MA-DDPG) can be trained by calling the ma_ddpg function defined bellow. Both agent and model classes are invoked from their respective helper files.\n",
    "\n",
    "**Note:** When training the environment, set `train_mode=True`, so that the line for resetting the environment looks like the following:\n",
    "```python\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ma_ddpg_agent import Agent\n",
    "import random\n",
    "import torch\n",
    "from collections import deque\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the agents\n",
    "agent1 = Agent(num_agents=1, state_size=state_size, action_size=action_size, random_seed=0)\n",
    "agent2 = Agent(num_agents=1, state_size=state_size, action_size=action_size, random_seed=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-Agent Deep Deterministic Policy Gradient(MA-DDPG)\n",
    "\n",
    "This solution makes use of the approach presented in the paper \"Multi-Agent Actor-Critic for Mixed Cooperative-Competitive Environments\". We use Actor-Critic networks, same as Deep deterministic policy gradients post, but instead of training each agent to learn from its own action, we incorporate actions taken by all agents. Why use actions taken by all agents to train the single agent? Because, the environment state depends on the actions taken by all agents (i.e non-stationary from a single agent's perspective) so if we just train the agent using its own action, the policy network does not get enough information to come up with a good policy and most likely will take longer to find any good policy.\n",
    "\n",
    "![MA-DDPG Algorithm](https://miro.medium.com/max/747/1*sfPu94LSl9nSKkvGIx-msw.png)\n",
    "\n",
    "Going back to Multi-Agent Actor-Critic paper, the action from each agent is used only during the training phase to ease the training (centralised training). During execution, we just use the the policy network that returns the action for a given state. We do not use any information from other agents (i.e. decentralised execution).\n",
    "\n",
    "![Training Phase](https://www.katnoria.com/static/f2e485d3a562562c8ce6600e4c930ef6/02ec2/maddpg_overview.png)\n",
    "\n",
    "This approach builds upon Deep Deterministic Policy Gradients(DDPG) and Deep Q-Network(DQN), so it is best to review some of the core concepts such as experience replay, fixed target network and actior-critic there. We will go over them rather quickly here.\n",
    "\n",
    "- The Actor network takes state as input and returns the action to take in the environment\n",
    "- The Critic network, Deep Q-Network, takes the state and action of all agents as input and returns the action-value (Q-value)\n",
    "- The Critic and Actor networks are trained by sampling experiences from the replay buffer.\n",
    "- The action-value from Critic is used to teach the Actor to choose better actions.\n",
    "\n",
    "The key improvement over the DDPG approach is that we now share the actions taken by all agents to train each agent.\n",
    "\n",
    "#### Network Architecture\n",
    "\n",
    "![Network Achitecture](https://www.katnoria.com/static/91ab09cf86f38ac97ebd3279cf47c618/5593c/ma_ddpg_architecture.png)\n",
    "\n",
    "Credits to Katnoria - \"Continuos Control With Deep Reinforcement Learning\"\n",
    "\n",
    "#### Hyperparemeters\n",
    "\n",
    "I tried several hyperparameters in order to solve the environment and the following worked the best for me.\n",
    "\n",
    "    - BUFFER_SIZE = int(1e6)      replay buffer size\n",
    "    - BATCH_SIZE = 128            minibatch size\n",
    "    - GAMMA = 0.99                discount factor\n",
    "    - TAU = 8e-2                  for soft update of target parameters\n",
    "    - LR_ACTOR = 1e-3             learning rate of the actor \n",
    "    - LR_CRITIC = 1e-3            learning rate of the critic\n",
    "    - WEIGHT_DECAY = 0            L2 weight decay\n",
    "    - UPDATE_EVERY = 1            how often to update the network\n",
    "    - UPDATE_NUM = 2              number of update passes\n",
    "    - EPSILON_START = 5.0         starting value of epsilon(noise decay process)\n",
    "    - EPSILON_END = 300           episode number to end the noise decay process\n",
    "    - EPSILON_THRESHOLD = 0       threshold value for epsilon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ma_ddpg(n_episodes=2000, max_timestep=1000, print_every=10, score_goal=0.5, limit_episodes=300,\n",
    "            agent1_actor_path='agent1_actor_checkpoint.pth', agent1_critic_path='agent1_critic_checkpoint.pth',\n",
    "            agent2_actor_path='agent2_actor_checkpoint.pth', agent2_critic_path='agent2_critic_checkpoint.pth'):\n",
    "    \"\"\"Multi-Agent Deep Deterministic Policy Gradient (MA-DDPG)\n",
    "\n",
    "    Params\n",
    "    ======\n",
    "        n_episodes (int)         : maximum number of training episodes\n",
    "        max_timestep (int)       : maximum number of timesteps per episode\n",
    "        print_every (int)        : print results every print_every number of episodes\n",
    "        score_goal (float)       : average reward over the last 100 episodes to solve environment\n",
    "        limit_episodes (int)     : stop training after limit_episodes without converging\n",
    "        agent1_actor_path (str)  : directory to store agent 1 actor network weights\n",
    "        agent1_critic_path (str) : directory to store agent 1 critic network weights\n",
    "        agent2_actor_path (str)  : directory to store agent 2 actor network weights\n",
    "        agent2_critic_path (str) : directory to store agent 2 critic network weights\n",
    "\n",
    "    \"\"\"\n",
    "    all_scores = []                      # initialize list for the mean scores from each episode\n",
    "    moving_avgs = []                     # initialize list for the moving averages of the episodes\n",
    "    scores_window = deque(maxlen=100)    # initialize a container for the mean scores of the last 100 episodes\n",
    "    best_score = -np.inf\n",
    "    best_episode = 0\n",
    "    solved = False\n",
    "    \n",
    "    for i_episode in range(1, n_episodes+1):\n",
    "        env_info = env.reset(train_mode=True)[brain_name]   # reset the environment\n",
    "        # get the current states and combine them\n",
    "        states = np.reshape(env_info.vector_observations, (1, state_size*num_agents))\n",
    "        scores = np.zeros(num_agents)                       # initialize the score (for each agent)\n",
    "        agent1.reset()\n",
    "        agent2.reset()\n",
    "        while True:\n",
    "            # get actions for each agent and combine them (add_noise=True)\n",
    "            actions = np.concatenate((agent1.act(states), agent2.act(states)), axis=0).flatten()\n",
    "            env_info = env.step(actions)[brain_name]        # send actions to environment\n",
    "            # get next state (for each agent)\n",
    "            next_states = np.reshape(env_info.vector_observations, (1, state_size*num_agents))\n",
    "            rewards = env_info.rewards                      # get reward (for each agent)\n",
    "            done = env_info.local_done                      # see if episode finished\n",
    "            # learning step for each agent\n",
    "            agent1.step(states, actions, rewards[0], next_states, done, 0)\n",
    "            agent2.step(states, actions, rewards[1], next_states, done, 1)\n",
    "            # save experience to replay buffer and perform learning step at defined interval\n",
    "            scores += np.max(rewards)                       # update the score (for each agent)\n",
    "            states = next_states                            # roll over states to next time step\n",
    "            if np.any(done):                                # exit loop if episode finished\n",
    "                break\n",
    "                \n",
    "        episode_best_score = np.max(scores)                 # select best score from the episode\n",
    "        scores_window.append(episode_best_score)            # save best score from the episode(100 best scores)\n",
    "        all_scores.append(episode_best_score)               # save best score from the episode\n",
    "        moving_avgs.append(np.mean(scores_window))          # save moving average\n",
    "        \n",
    "        # save best score                        \n",
    "        if episode_best_score > best_score:\n",
    "            best_score = episode_best_score\n",
    "            best_episode = i_episode\n",
    "            \n",
    "        # print results every print_every episodes\n",
    "        if i_episode % print_every == 0:\n",
    "            print('\\rEpisode {} : Moving Average - {:.2f} | Min score - {:.2f} | Max score - {:.2f}'\n",
    "                  .format(i_episode, moving_avgs[-1],\n",
    "                          np.min(all_scores[-print_every:]), np.max(all_scores[-print_every:])))\n",
    "        \n",
    "        # check if environment is solved and save best models\n",
    "        if moving_avgs[-1] >= score_goal and i_episode >= 100:\n",
    "            if not solved:\n",
    "                print('\\rEnvironment solved in {} : Moving Average - {:.2f} | Min score - {:.2f} | Max score - {:.2f}\\n'\n",
    "                      .format(i_episode-100, moving_avgs[-1],\n",
    "                              np.min(all_scores[-print_every:]), np.max(all_scores[-print_every:])))\n",
    "                solved = True\n",
    "                # save weights for the models when environment is solved\n",
    "                torch.save(agent1.actor_local.state_dict(), agent1_actor_path)\n",
    "                torch.save(agent1.critic_local.state_dict(), agent1_critic_path)\n",
    "                torch.save(agent2.actor_local.state_dict(), agent2_actor_path)\n",
    "                torch.save(agent2.critic_local.state_dict(), agent2_critic_path)\n",
    "            elif episode_best_score >= best_score:\n",
    "                print('\\rBest episode so far {} : Moving Average - {:.2f} | Best score - {:.2f}\\n'\n",
    "                      .format(i_episode, moving_avgs[-1], episode_best_score))\n",
    "                # save weights for the models if score improves\n",
    "                torch.save(agent1.actor_local.state_dict(), agent1_actor_path)\n",
    "                torch.save(agent1.critic_local.state_dict(), agent1_critic_path)\n",
    "                torch.save(agent2.actor_local.state_dict(), agent2_actor_path)\n",
    "                torch.save(agent2.critic_local.state_dict(), agent2_critic_path)\n",
    "            elif (i_episode-best_episode) >= limit_episodes:\n",
    "                # stop training if the model stops converging\n",
    "                print('##### TRAINING STOPPED #####\\nBest score not matched or exceeded for {} episodes'\n",
    "                      .format(limit_episodes))\n",
    "                break\n",
    "            else:\n",
    "                continue\n",
    "                \n",
    "    return all_scores, moving_avgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 10 : Moving Average - 0.00 | Min score - 0.00 | Max score - 0.00\n",
      "Episode 20 : Moving Average - 0.02 | Min score - 0.00 | Max score - 0.10\n",
      "Episode 30 : Moving Average - 0.01 | Min score - 0.00 | Max score - 0.00\n",
      "Episode 40 : Moving Average - 0.02 | Min score - 0.00 | Max score - 0.10\n",
      "Episode 50 : Moving Average - 0.01 | Min score - 0.00 | Max score - 0.10\n",
      "Episode 60 : Moving Average - 0.01 | Min score - 0.00 | Max score - 0.00\n",
      "Episode 70 : Moving Average - 0.02 | Min score - 0.00 | Max score - 0.20\n",
      "Episode 80 : Moving Average - 0.02 | Min score - 0.00 | Max score - 0.10\n",
      "Episode 90 : Moving Average - 0.02 | Min score - 0.00 | Max score - 0.20\n",
      "Episode 100 : Moving Average - 0.02 | Min score - 0.00 | Max score - 0.20\n",
      "Episode 110 : Moving Average - 0.02 | Min score - 0.00 | Max score - 0.10\n",
      "Episode 120 : Moving Average - 0.03 | Min score - 0.00 | Max score - 0.20\n",
      "Episode 130 : Moving Average - 0.03 | Min score - 0.00 | Max score - 0.20\n",
      "Episode 140 : Moving Average - 0.03 | Min score - 0.00 | Max score - 0.40\n",
      "Episode 150 : Moving Average - 0.03 | Min score - 0.00 | Max score - 0.10\n",
      "Episode 160 : Moving Average - 0.04 | Min score - 0.00 | Max score - 0.20\n",
      "Episode 170 : Moving Average - 0.04 | Min score - 0.00 | Max score - 0.40\n",
      "Episode 180 : Moving Average - 0.05 | Min score - 0.00 | Max score - 0.30\n",
      "Episode 190 : Moving Average - 0.05 | Min score - 0.00 | Max score - 0.30\n",
      "Episode 200 : Moving Average - 0.05 | Min score - 0.00 | Max score - 0.20\n",
      "Episode 210 : Moving Average - 0.06 | Min score - 0.00 | Max score - 0.10\n",
      "Episode 220 : Moving Average - 0.06 | Min score - 0.00 | Max score - 0.20\n",
      "Episode 230 : Moving Average - 0.06 | Min score - 0.00 | Max score - 0.10\n",
      "Episode 240 : Moving Average - 0.07 | Min score - 0.00 | Max score - 0.40\n",
      "Episode 250 : Moving Average - 0.07 | Min score - 0.00 | Max score - 0.30\n",
      "Episode 260 : Moving Average - 0.08 | Min score - 0.00 | Max score - 0.30\n",
      "Episode 270 : Moving Average - 0.08 | Min score - 0.00 | Max score - 0.20\n",
      "Episode 280 : Moving Average - 0.08 | Min score - 0.00 | Max score - 0.30\n",
      "Episode 290 : Moving Average - 0.09 | Min score - 0.10 | Max score - 0.20\n",
      "Episode 300 : Moving Average - 0.09 | Min score - 0.00 | Max score - 0.20\n",
      "Episode 310 : Moving Average - 0.11 | Min score - 0.00 | Max score - 0.30\n",
      "Episode 320 : Moving Average - 0.11 | Min score - 0.10 | Max score - 0.30\n",
      "Episode 330 : Moving Average - 0.12 | Min score - 0.10 | Max score - 0.30\n",
      "Episode 340 : Moving Average - 0.12 | Min score - 0.10 | Max score - 0.20\n",
      "Episode 350 : Moving Average - 0.13 | Min score - 0.00 | Max score - 0.40\n",
      "Episode 360 : Moving Average - 0.14 | Min score - 0.00 | Max score - 0.70\n",
      "Episode 370 : Moving Average - 0.15 | Min score - 0.00 | Max score - 0.40\n",
      "Episode 380 : Moving Average - 0.16 | Min score - 0.00 | Max score - 0.50\n",
      "Episode 390 : Moving Average - 0.16 | Min score - 0.00 | Max score - 0.30\n",
      "Episode 400 : Moving Average - 0.17 | Min score - 0.10 | Max score - 0.30\n",
      "Episode 410 : Moving Average - 0.17 | Min score - 0.10 | Max score - 0.30\n",
      "Episode 420 : Moving Average - 0.18 | Min score - 0.10 | Max score - 0.50\n",
      "Episode 430 : Moving Average - 0.18 | Min score - 0.10 | Max score - 0.30\n",
      "Episode 440 : Moving Average - 0.18 | Min score - 0.10 | Max score - 0.30\n",
      "Episode 450 : Moving Average - 0.18 | Min score - 0.00 | Max score - 0.50\n",
      "Episode 460 : Moving Average - 0.17 | Min score - 0.00 | Max score - 0.20\n",
      "Episode 470 : Moving Average - 0.16 | Min score - 0.00 | Max score - 0.20\n",
      "Episode 480 : Moving Average - 0.15 | Min score - 0.00 | Max score - 0.30\n",
      "Episode 490 : Moving Average - 0.15 | Min score - 0.00 | Max score - 0.40\n",
      "Episode 500 : Moving Average - 0.15 | Min score - 0.10 | Max score - 0.40\n",
      "Episode 510 : Moving Average - 0.15 | Min score - 0.10 | Max score - 0.30\n",
      "Episode 520 : Moving Average - 0.14 | Min score - 0.09 | Max score - 0.20\n",
      "Episode 530 : Moving Average - 0.13 | Min score - 0.00 | Max score - 0.50\n",
      "Episode 540 : Moving Average - 0.14 | Min score - 0.00 | Max score - 0.60\n",
      "Episode 550 : Moving Average - 0.14 | Min score - 0.10 | Max score - 0.30\n",
      "Episode 560 : Moving Average - 0.16 | Min score - 0.00 | Max score - 0.50\n",
      "Episode 570 : Moving Average - 0.17 | Min score - 0.00 | Max score - 0.50\n",
      "Episode 580 : Moving Average - 0.19 | Min score - 0.10 | Max score - 0.40\n",
      "Episode 590 : Moving Average - 0.21 | Min score - 0.10 | Max score - 1.00\n",
      "Episode 600 : Moving Average - 0.21 | Min score - 0.00 | Max score - 0.30\n",
      "Episode 610 : Moving Average - 0.21 | Min score - 0.10 | Max score - 0.40\n",
      "Episode 620 : Moving Average - 0.22 | Min score - 0.10 | Max score - 0.60\n",
      "Episode 630 : Moving Average - 0.22 | Min score - 0.10 | Max score - 0.40\n",
      "Episode 640 : Moving Average - 0.22 | Min score - 0.00 | Max score - 0.50\n",
      "Episode 650 : Moving Average - 0.21 | Min score - 0.00 | Max score - 0.30\n",
      "Episode 660 : Moving Average - 0.20 | Min score - 0.00 | Max score - 0.50\n",
      "Episode 670 : Moving Average - 0.20 | Min score - 0.00 | Max score - 0.60\n",
      "Episode 680 : Moving Average - 0.19 | Min score - 0.00 | Max score - 0.40\n",
      "Episode 690 : Moving Average - 0.16 | Min score - 0.00 | Max score - 0.30\n",
      "Episode 700 : Moving Average - 0.17 | Min score - 0.00 | Max score - 1.10\n",
      "Episode 710 : Moving Average - 0.18 | Min score - 0.00 | Max score - 0.40\n",
      "Episode 720 : Moving Average - 0.17 | Min score - 0.00 | Max score - 0.40\n",
      "Episode 730 : Moving Average - 0.17 | Min score - 0.00 | Max score - 0.50\n",
      "Episode 740 : Moving Average - 0.17 | Min score - 0.00 | Max score - 0.30\n",
      "Episode 750 : Moving Average - 0.17 | Min score - 0.00 | Max score - 0.40\n",
      "Episode 760 : Moving Average - 0.18 | Min score - 0.00 | Max score - 1.00\n",
      "Episode 770 : Moving Average - 0.17 | Min score - 0.00 | Max score - 0.50\n",
      "Episode 780 : Moving Average - 0.16 | Min score - 0.00 | Max score - 0.20\n",
      "Episode 790 : Moving Average - 0.17 | Min score - 0.10 | Max score - 0.40\n",
      "Episode 800 : Moving Average - 0.16 | Min score - 0.00 | Max score - 0.30\n",
      "Episode 810 : Moving Average - 0.16 | Min score - 0.00 | Max score - 0.90\n",
      "Episode 820 : Moving Average - 0.17 | Min score - 0.10 | Max score - 0.50\n",
      "Episode 830 : Moving Average - 0.17 | Min score - 0.10 | Max score - 0.60\n",
      "Episode 840 : Moving Average - 0.19 | Min score - 0.00 | Max score - 1.00\n",
      "Episode 850 : Moving Average - 0.21 | Min score - 0.00 | Max score - 1.20\n",
      "Episode 860 : Moving Average - 0.22 | Min score - 0.10 | Max score - 0.90\n",
      "Episode 870 : Moving Average - 0.24 | Min score - 0.10 | Max score - 1.00\n",
      "Episode 880 : Moving Average - 0.26 | Min score - 0.10 | Max score - 0.59\n",
      "Episode 890 : Moving Average - 0.26 | Min score - 0.10 | Max score - 0.40\n",
      "Episode 900 : Moving Average - 0.26 | Min score - 0.10 | Max score - 0.30\n",
      "Episode 910 : Moving Average - 0.27 | Min score - 0.10 | Max score - 0.50\n",
      "Episode 920 : Moving Average - 0.30 | Min score - 0.10 | Max score - 1.40\n",
      "Episode 930 : Moving Average - 0.34 | Min score - 0.10 | Max score - 2.30\n",
      "Episode 940 : Moving Average - 0.32 | Min score - 0.10 | Max score - 0.30\n",
      "Episode 950 : Moving Average - 0.33 | Min score - 0.10 | Max score - 1.40\n",
      "Episode 960 : Moving Average - 0.36 | Min score - 0.10 | Max score - 3.80\n",
      "Episode 970 : Moving Average - 0.33 | Min score - 0.10 | Max score - 0.60\n",
      "Episode 980 : Moving Average - 0.36 | Min score - 0.10 | Max score - 1.30\n",
      "Episode 990 : Moving Average - 0.40 | Min score - 0.10 | Max score - 1.90\n",
      "Episode 1000 : Moving Average - 0.40 | Min score - 0.10 | Max score - 0.60\n",
      "Episode 1010 : Moving Average - 0.40 | Min score - 0.10 | Max score - 0.40\n",
      "Episode 1020 : Moving Average - 0.40 | Min score - 0.10 | Max score - 1.60\n",
      "Episode 1030 : Moving Average - 0.39 | Min score - 0.10 | Max score - 2.10\n",
      "Episode 1040 : Moving Average - 0.42 | Min score - 0.00 | Max score - 3.70\n",
      "Episode 1050 : Moving Average - 0.43 | Min score - 0.10 | Max score - 1.80\n",
      "Episode 1060 : Moving Average - 0.41 | Min score - 0.10 | Max score - 1.00\n",
      "Episode 1070 : Moving Average - 0.43 | Min score - 0.10 | Max score - 0.80\n",
      "Episode 1080 : Moving Average - 0.49 | Min score - 0.10 | Max score - 4.80\n",
      "Episode 1090 : Moving Average - 0.46 | Min score - 0.00 | Max score - 1.30\n",
      "Environment solved in 994 : Moving Average - 0.52 | Min score - 0.00 | Max score - 5.20\n",
      "\n",
      "Episode 1100 : Moving Average - 0.60 | Min score - 0.00 | Max score - 5.20\n",
      "Episode 1110 : Moving Average - 0.74 | Min score - 0.10 | Max score - 3.80\n",
      "Episode 1120 : Moving Average - 0.90 | Min score - 0.10 | Max score - 5.00\n",
      "Episode 1130 : Moving Average - 0.93 | Min score - 0.10 | Max score - 3.50\n",
      "Best episode so far 1137 : Moving Average - 0.99 | Best score - 5.20\n",
      "\n",
      "Episode 1140 : Moving Average - 1.02 | Min score - 0.00 | Max score - 5.20\n",
      "Episode 1150 : Moving Average - 1.06 | Min score - 0.10 | Max score - 3.80\n",
      "Episode 1160 : Moving Average - 1.16 | Min score - 0.10 | Max score - 4.20\n",
      "Episode 1170 : Moving Average - 1.27 | Min score - 0.10 | Max score - 5.10\n",
      "Episode 1180 : Moving Average - 1.24 | Min score - 0.00 | Max score - 3.90\n",
      "Episode 1190 : Moving Average - 1.28 | Min score - 0.10 | Max score - 1.50\n",
      "Episode 1200 : Moving Average - 1.24 | Min score - 0.10 | Max score - 4.20\n",
      "Best episode so far 1202 : Moving Average - 1.28 | Best score - 5.20\n",
      "\n",
      "Episode 1210 : Moving Average - 1.35 | Min score - 0.80 | Max score - 5.20\n",
      "Episode 1220 : Moving Average - 1.28 | Min score - 0.10 | Max score - 2.30\n",
      "Episode 1230 : Moving Average - 1.35 | Min score - 0.10 | Max score - 4.70\n",
      "Episode 1240 : Moving Average - 1.29 | Min score - 0.10 | Max score - 2.80\n",
      "Episode 1250 : Moving Average - 1.33 | Min score - 0.10 | Max score - 2.80\n",
      "Episode 1260 : Moving Average - 1.33 | Min score - 0.00 | Max score - 4.00\n",
      "Episode 1270 : Moving Average - 1.31 | Min score - 0.10 | Max score - 5.10\n",
      "Episode 1280 : Moving Average - 1.30 | Min score - 0.00 | Max score - 4.40\n",
      "Episode 1290 : Moving Average - 1.30 | Min score - 0.00 | Max score - 3.10\n",
      "Episode 1300 : Moving Average - 1.23 | Min score - 0.00 | Max score - 1.70\n",
      "Episode 1310 : Moving Average - 1.06 | Min score - 0.10 | Max score - 3.20\n",
      "Episode 1320 : Moving Average - 1.04 | Min score - 0.00 | Max score - 4.80\n",
      "Best episode so far 1328 : Moving Average - 1.00 | Best score - 5.20\n",
      "\n",
      "Episode 1330 : Moving Average - 1.02 | Min score - 0.20 | Max score - 5.20\n",
      "Episode 1340 : Moving Average - 1.08 | Min score - 0.20 | Max score - 3.40\n",
      "Episode 1350 : Moving Average - 1.10 | Min score - 0.10 | Max score - 5.10\n",
      "Best episode so far 1354 : Moving Average - 1.21 | Best score - 5.20\n",
      "\n",
      "Episode 1360 : Moving Average - 1.15 | Min score - 0.10 | Max score - 5.20\n",
      "Episode 1370 : Moving Average - 1.09 | Min score - 0.10 | Max score - 2.20\n",
      "Episode 1380 : Moving Average - 1.07 | Min score - 0.10 | Max score - 1.50\n",
      "Best episode so far 1384 : Moving Average - 1.11 | Best score - 5.20\n",
      "\n",
      "Episode 1390 : Moving Average - 1.09 | Min score - 0.00 | Max score - 5.20\n",
      "##### TRAINING STOPPED #####\n",
      "Best score not matched or exceeded for 300 episodes\n"
     ]
    }
   ],
   "source": [
    "from workspace_utils import active_session\n",
    "\n",
    "with active_session():  # used to keep Udacity's workspace awake during training\n",
    "    # train the agents\n",
    "    scores, moving_avgs = ma_ddpg()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The environment is considered solved when the average reward over 100 episode is +0.5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7ff3d9419550>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot the scores\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot()\n",
    "plt.plot(np.arange(len(scores)), scores, label='Reward')\n",
    "plt.plot(np.arange(len(scores)), moving_avgs, color='y', label='Moving Average')\n",
    "plt.axhline(0.5, color=\"r\", linestyle='dashed', label='Target Score')\n",
    "plt.ylabel('Score')\n",
    "plt.xlabel('Episode #')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When finished, you can close the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
