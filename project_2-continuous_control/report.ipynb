{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Continuous Control\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "## 1. Start the Environment\n",
    "\n",
    "Run the next code cell to install a few packages.  This line will take a few minutes to run!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mtensorflow 1.7.1 has requirement numpy>=1.13.3, but you'll have numpy 1.12.1 which is incompatible.\u001b[0m\r\n",
      "\u001b[31mipython 6.5.0 has requirement prompt-toolkit<2.0.0,>=1.0.15, but you'll have prompt-toolkit 2.0.10 which is incompatible.\u001b[0m\r\n"
     ]
    }
   ],
   "source": [
    "!pip -q install ./python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The environments corresponding to both versions of the environment can be accessed at the file paths provided below.  \n",
    "\n",
    "Please select one of the two options below for loading the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\tgoal_size -> 5.0\n",
      "\t\tgoal_speed -> 1.0\n",
      "Unity brain name: ReacherBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 33\n",
      "        Number of stacked Vector Observation: 1\n",
      "        Vector Action space type: continuous\n",
      "        Vector Action space size (per agent): 4\n",
      "        Vector Action descriptions: , , , \n"
     ]
    }
   ],
   "source": [
    "from unityagents import UnityEnvironment\n",
    "import numpy as np\n",
    "\n",
    "# select this option to load version 1 (with a single agent) of the environment\n",
    "# env = UnityEnvironment(file_name='/data/Reacher_One_Linux_NoVis/Reacher_One_Linux_NoVis.x86_64')\n",
    "\n",
    "# select this option to load version 2 (with 20 agents) of the environment\n",
    "env = UnityEnvironment(file_name='/data/Reacher_Linux_NoVis/Reacher.x86_64')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Environments contain **_brains_** which are responsible for deciding the actions of their associated agents. Here we check for the first brain available, and set it as the default brain we will be controlling from Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the default brain\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Examine the State and Action Spaces\n",
    "\n",
    "Run the code cell below to print some information about the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of agents: 20\n",
      "Size of each action: 4\n",
      "There are 20 agents. Each observes a state with length: 33\n",
      "The state for the first agent looks like: [  0.00000000e+00  -4.00000000e+00   0.00000000e+00   1.00000000e+00\n",
      "  -0.00000000e+00  -0.00000000e+00  -4.37113883e-08   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00  -1.00000000e+01   0.00000000e+00\n",
      "   1.00000000e+00  -0.00000000e+00  -0.00000000e+00  -4.37113883e-08\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   5.75471878e+00  -1.00000000e+00\n",
      "   5.55726624e+00   0.00000000e+00   1.00000000e+00   0.00000000e+00\n",
      "  -1.68164849e-01]\n"
     ]
    }
   ],
   "source": [
    "# reset the environment\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "\n",
    "# number of agents\n",
    "num_agents = len(env_info.agents)\n",
    "print('Number of agents:', num_agents)\n",
    "\n",
    "# size of each action\n",
    "action_size = brain.vector_action_space_size\n",
    "print('Size of each action:', action_size)\n",
    "\n",
    "# examine the state space \n",
    "states = env_info.vector_observations\n",
    "state_size = states.shape[1]\n",
    "print('There are {} agents. Each observes a state with length: {}'.format(states.shape[0], state_size))\n",
    "print('The state for the first agent looks like:', states[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Reacher](https://github.com/Unity-Technologies/ml-agents/blob/master/docs/images/reacher.png?raw=true)\n",
    "\n",
    "* Set-up: Double-jointed arm which can move to target locations.\n",
    "* Goal: The agents must move its hand to the goal location, and keep it there.\n",
    "* Agents: The environment contains 20 agents linked to a single brain.\n",
    "* Agent Reward Function (independent):\n",
    "  * +0.1 Each step agent's hand is in goal location.\n",
    "* Brains: One Brain with the following observation/action space.\n",
    "  * Vector Observation space: 33 variables corresponding to position, rotation,\n",
    "    velocity, and angular velocities of the two arm Rigidbodies.\n",
    "  * Vector Action space: (Continuous) Size of 4, corresponding to torque\n",
    "    applicable to two joints. Every entry in the action vector should be a \n",
    "    number between -1 and 1.\n",
    "  * Visual Observations: None.\n",
    "* Reset Parameters: Five\n",
    "  * goal_size: radius of the goal zone\n",
    "    * Default: 5\n",
    "    * Recommended Minimum: 1\n",
    "    * Recommended Maximum: 10\n",
    "  * goal_speed: speed of the goal zone around the arm (in radians)\n",
    "    * Default: 1\n",
    "    * Recommended Minimum: 0.2\n",
    "    * Recommended Maximum: 4\n",
    "  * gravity\n",
    "    * Default: 9.81\n",
    "    * Recommended Minimum: 4\n",
    "    * Recommended Maximum: 20\n",
    "  * deviation: Magnitude of sinusoidal (cosine) deviation of the goal along the vertical dimension\n",
    "    * Default: 0\n",
    "    * Recommended Minimum: 0\n",
    "    * Recommended Maximum: 5\n",
    "  * deviation_freq: Frequency of the cosine deviation of the goal along the vertical dimension\n",
    "    * Default: 0\n",
    "    * Recommended Minimum: 0\n",
    "    * Recommended Maximum: 3\n",
    "* Benchmark Mean Reward: 30"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Take Random Actions in the Environment\n",
    "\n",
    "In the next code cell, you will learn how to use the Python API to control the agent and receive feedback from the environment.\n",
    "\n",
    "**Note:** you should set `train_mode=True` to restart the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total score (averaged over agents) this episode: 0.13649999694898723\n"
     ]
    }
   ],
   "source": [
    "env_info = env.reset(train_mode=True)[brain_name]      # reset the environment    \n",
    "states = env_info.vector_observations                  # get the current state (for each agent)\n",
    "scores = np.zeros(num_agents)                          # initialize the score (for each agent)\n",
    "while True:\n",
    "    actions = np.random.randn(num_agents, action_size) # select an action (for each agent)\n",
    "    actions = np.clip(actions, -1, 1)                  # all actions between -1 and 1\n",
    "    env_info = env.step(actions)[brain_name]           # send all actions to the environment\n",
    "    next_states = env_info.vector_observations         # get next state (for each agent)\n",
    "    rewards = env_info.rewards                         # get reward (for each agent)\n",
    "    dones = env_info.local_done                        # see if episode finished\n",
    "    scores += env_info.rewards                         # update the score (for each agent)\n",
    "    states = next_states                               # roll over states to next time step\n",
    "    if np.any(dones):                                  # exit loop if episode finished\n",
    "        break\n",
    "print('Total score (averaged over agents) this episode: {}'.format(np.mean(scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Train the agents\n",
    "\n",
    "The DDPG can be trained by calling the ddpg function defined bellow. Both agent and model classes are invoked from their respective helper files.\n",
    "\n",
    "**Note:**\n",
    "When training the environment, set `train_mode=True`, so that the line for resetting the environment looks like the following:\n",
    "```python\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ddpg_agent import Agent\n",
    "import random\n",
    "import torch\n",
    "from collections import deque\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the agent\n",
    "agent = Agent(state_size=state_size, action_size=action_size, random_seed=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyperparameters\n",
    "\n",
    "The following hyperparameters were used to train the agents:\n",
    "    - BUFFER_SIZE = int(1e6)       replay buffer size\n",
    "    - BATCH_SIZE = 128             minibatch size\n",
    "    - GAMMA = 0.99                 discount factor\n",
    "    - TAU = 1e-3                   for soft update of target parameters\n",
    "    - LR_ACTOR = 1e-4              learning rate of the actor \n",
    "    - LR_CRITIC = 3e-4             learning rate of the critic\n",
    "    - WEIGHT_DECAY = 0             L2 weight decay\n",
    "    - UPDATE_EVERY = 20            how often to update the network\n",
    "    - UPDATE_NUM = 15              number of update passes\n",
    "    - EPSILON = 1.0                starting value of epsilon\n",
    "    - EPSILON_DECAY = 1e-6         factor for decreasing epsilon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deep Deterministic Policy Gradient (DDPG)\n",
    "\n",
    "Deep Deterministic Policy Gradient (DDPG) is an algorithm which concurrently learns a Q-function and a policy. It uses off-policy data and the Bellman equation to learn the Q-function, and uses the Q-function to learn the policy.\n",
    "\n",
    "This approach is closely connected to Q-learning, and is motivated the same way: if you know the optimal action-value function Q* (s,a), then in any given state, the optimal action a* (s) can be found by solving\n",
    "\n",
    "![Optimal Action](https://spinningup.openai.com/en/latest/_images/math/ba36101c11fe8d42a8eb6367560792b6dc11a312.svg)\n",
    "\n",
    "DDPG interleaves learning an approximator to Q* (s,a) with learning an approximator to a* (s), and it does so in a way which is specifically adapted for environments with continuous action spaces. But what does it mean that DDPG is adapted specifically for environments with continuous action spaces? It relates to how we compute the max over actions in max_a Q* (s,a).\n",
    "\n",
    "When there are a finite number of discrete actions, the max poses no problem, because we can just compute the Q-values for each action separately and directly compare them. (This also immediately gives us the action which maximizes the Q-value.) But when the action space is continuous, we can’t exhaustively evaluate the space, and solving the optimization problem is highly non-trivial. Using a normal optimization algorithm would make calculating max_a Q* (s,a) a painfully expensive subroutine. And since it would need to be run every time the agent wants to take an action in the environment, this is unacceptable.\n",
    "\n",
    "Because the action space is continuous, the function Q* (s,a) is presumed to be differentiable with respect to the action argument. This allows us to set up an efficient, gradient-based learning rule for a policy u(s) which exploits that fact. Then, instead of running an expensive optimization subroutine each time we wish to compute max_a Q(s,a), we can approximate it with max_a Q(s,a) Q(s,u(s)).\n",
    "\n",
    "Credits to \"OpenAI - Spinning Up\"\n",
    "\n",
    "#### Quick Facts\n",
    "\n",
    "- DDPG is an off-policy algorithm.\n",
    "- DDPG can only be used for environments with continuous action spaces.\n",
    "- DDPG can be thought of as being deep Q-learning for continuous action spaces.\n",
    "\n",
    "#### Pseudocode\n",
    "\n",
    "![DDPG Pseudocode](https://spinningup.openai.com/en/latest/_images/math/66446594f760581068c5684f053e75f3de9b1404.svg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ddpg(n_episodes=1000, max_timestep=1000,\n",
    "         actor_ckpt_path='actor_checkpoint.pth', critic_ckpt_path='critic_checkpoint.pth'):\n",
    "    \"\"\"Deep Deterministic Policy Gradient (DDPG)\n",
    "\n",
    "    Params\n",
    "    ======\n",
    "        n_episodes (int)       : maximum number of training episodes\n",
    "        max_timestep (int)     : maximum number of timesteps per episode\n",
    "        actor_ckpt_path (str)  : directory to store actor network weights\n",
    "        critic_ckpt_path (str) : directory to store critic network weights\n",
    "\n",
    "    \"\"\"\n",
    "    mean_scores = []                     # initialize list for the mean scores from each episode\n",
    "    min_scores = []                      # initialize list for the lowest scores from each episode\n",
    "    max_scores = []                      # initialize list for the highest scores from each episode\n",
    "    moving_avgs = []                     # initialize list for the moving averages of the episodes\n",
    "    scores_window = deque(maxlen=100)    # initialize a container for the mean scores of the last 100 episodes\n",
    "    best_score = -np.inf\n",
    "    \n",
    "    for i_episode in range(1, n_episodes+1):\n",
    "        env_info = env.reset(train_mode=True)[brain_name]   # reset the environment \n",
    "        states = env_info.vector_observations               # get the current state (for each agent)  \n",
    "        scores = np.zeros(num_agents)                       # initialize the score (for each agent)\n",
    "        agent.reset()\n",
    "        for timestep in range(max_timestep):\n",
    "            actions = agent.act(states)                     # select an action (for each agent, add_noise=True)\n",
    "            env_info = env.step(actions)[brain_name]        # send actions to environment\n",
    "            next_states = env_info.vector_observations      # get next state (for each agent)\n",
    "            rewards = env_info.rewards                      # get reward (for each agent)\n",
    "            dones = env_info.local_done                     # see if episode finished\n",
    "            # save experience to replay buffer and perform learning step at defined interval\n",
    "            for state, action, reward, next_state, done in zip(states, actions, rewards, next_states, dones):\n",
    "                agent.step(state, action, reward, next_state, done, timestep)\n",
    "            scores += rewards                               # update the score (for each agent)\n",
    "            states = next_states                            # roll over states to next time step\n",
    "            if np.any(dones):                               # exit loop if episode finished\n",
    "                break\n",
    "\n",
    "        mean_scores.append(np.mean(scores))                 # save mean score for the episode\n",
    "        min_scores.append(np.min(scores))                   # save lowest score\n",
    "        max_scores.append(np.max(scores))                   # save highest score\n",
    "        scores_window.append(mean_scores[-1])               # save mean score to the 100 value window\n",
    "        moving_avgs.append(np.mean(scores_window))          # save moving average\n",
    "                \n",
    "        print('\\rEpisode {} : Average score - {:.2f} | Moving Average - {:.2f} | Min score - {:.2f} | Max score - {:.2f}'\n",
    "              .format(i_episode, mean_scores[-1], moving_avgs[-1], min_scores[-1], max_scores[-1]))\n",
    "                  \n",
    "        if moving_avgs[-1] >= 30.0 and i_episode >= 100:\n",
    "            print('\\nEnvironment solved in {} episodes! Moving Average = {:.2f} over last 100 episodes'\n",
    "                  .format(i_episode-100, moving_avgs[-1]))            \n",
    "            torch.save(agent.actor_local.state_dict(), actor_ckpt_path)\n",
    "            torch.save(agent.critic_local.state_dict(), critic_ckpt_path)  \n",
    "            break\n",
    "        \n",
    "        if mean_scores[-1] > best_score:\n",
    "            torch.save(agent.actor_local.state_dict(), actor_ckpt_path)\n",
    "            torch.save(agent.critic_local.state_dict(), critic_ckpt_path)\n",
    "            \n",
    "        if i_episode == n_episodes:\n",
    "            print('\\nMaximum number {} of episodes reached! Moving Average = {:.2f} over the last 100 episodes'\n",
    "                  .format(i_episode, moving_avgs[-1]))\n",
    "            \n",
    "    return mean_scores, moving_avgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1 : Average score - 1.57 | Moving Average - 1.57 | Min score - 0.31 | Max score - 4.40\n",
      "Episode 2 : Average score - 3.66 | Moving Average - 2.62 | Min score - 2.05 | Max score - 4.79\n",
      "Episode 3 : Average score - 4.08 | Moving Average - 3.11 | Min score - 2.22 | Max score - 7.28\n",
      "Episode 4 : Average score - 5.85 | Moving Average - 3.79 | Min score - 3.94 | Max score - 7.76\n",
      "Episode 5 : Average score - 6.80 | Moving Average - 4.39 | Min score - 3.60 | Max score - 9.89\n",
      "Episode 6 : Average score - 7.62 | Moving Average - 4.93 | Min score - 4.70 | Max score - 12.00\n",
      "Episode 7 : Average score - 8.01 | Moving Average - 5.37 | Min score - 5.81 | Max score - 12.90\n",
      "Episode 8 : Average score - 11.00 | Moving Average - 6.07 | Min score - 7.20 | Max score - 15.75\n",
      "Episode 9 : Average score - 15.25 | Moving Average - 7.09 | Min score - 11.18 | Max score - 20.53\n",
      "Episode 10 : Average score - 17.71 | Moving Average - 8.16 | Min score - 12.05 | Max score - 22.97\n",
      "Episode 11 : Average score - 20.20 | Moving Average - 9.25 | Min score - 12.65 | Max score - 35.33\n",
      "Episode 12 : Average score - 23.79 | Moving Average - 10.46 | Min score - 14.25 | Max score - 35.62\n",
      "Episode 13 : Average score - 24.50 | Moving Average - 11.54 | Min score - 17.48 | Max score - 29.01\n",
      "Episode 14 : Average score - 27.08 | Moving Average - 12.65 | Min score - 20.51 | Max score - 31.27\n",
      "Episode 15 : Average score - 26.73 | Moving Average - 13.59 | Min score - 17.48 | Max score - 33.64\n",
      "Episode 16 : Average score - 28.21 | Moving Average - 14.50 | Min score - 22.24 | Max score - 34.48\n",
      "Episode 17 : Average score - 29.99 | Moving Average - 15.41 | Min score - 19.30 | Max score - 34.96\n",
      "Episode 18 : Average score - 32.16 | Moving Average - 16.35 | Min score - 20.26 | Max score - 37.38\n",
      "Episode 19 : Average score - 33.87 | Moving Average - 17.27 | Min score - 26.70 | Max score - 39.31\n",
      "Episode 20 : Average score - 35.19 | Moving Average - 18.16 | Min score - 27.05 | Max score - 38.64\n",
      "Episode 21 : Average score - 35.27 | Moving Average - 18.98 | Min score - 30.38 | Max score - 39.19\n",
      "Episode 22 : Average score - 34.37 | Moving Average - 19.68 | Min score - 26.10 | Max score - 39.44\n",
      "Episode 23 : Average score - 36.76 | Moving Average - 20.42 | Min score - 29.94 | Max score - 39.35\n",
      "Episode 24 : Average score - 37.43 | Moving Average - 21.13 | Min score - 33.77 | Max score - 39.45\n",
      "Episode 25 : Average score - 37.43 | Moving Average - 21.78 | Min score - 31.39 | Max score - 39.30\n",
      "Episode 26 : Average score - 37.32 | Moving Average - 22.38 | Min score - 32.29 | Max score - 39.36\n",
      "Episode 27 : Average score - 36.40 | Moving Average - 22.90 | Min score - 29.32 | Max score - 39.23\n",
      "Episode 28 : Average score - 37.13 | Moving Average - 23.41 | Min score - 31.71 | Max score - 39.59\n",
      "Episode 29 : Average score - 37.46 | Moving Average - 23.89 | Min score - 34.10 | Max score - 39.41\n",
      "Episode 30 : Average score - 38.16 | Moving Average - 24.37 | Min score - 34.69 | Max score - 39.55\n",
      "Episode 31 : Average score - 37.04 | Moving Average - 24.78 | Min score - 33.52 | Max score - 39.53\n",
      "Episode 32 : Average score - 36.56 | Moving Average - 25.14 | Min score - 29.68 | Max score - 39.08\n",
      "Episode 33 : Average score - 36.78 | Moving Average - 25.50 | Min score - 32.44 | Max score - 39.48\n",
      "Episode 34 : Average score - 37.57 | Moving Average - 25.85 | Min score - 31.58 | Max score - 39.56\n",
      "Episode 35 : Average score - 38.10 | Moving Average - 26.20 | Min score - 34.40 | Max score - 39.61\n",
      "Episode 36 : Average score - 37.84 | Moving Average - 26.52 | Min score - 31.29 | Max score - 39.59\n",
      "Episode 37 : Average score - 36.71 | Moving Average - 26.80 | Min score - 31.12 | Max score - 39.41\n",
      "Episode 38 : Average score - 37.42 | Moving Average - 27.08 | Min score - 33.23 | Max score - 39.59\n",
      "Episode 39 : Average score - 38.39 | Moving Average - 27.37 | Min score - 36.26 | Max score - 39.55\n",
      "Episode 40 : Average score - 38.91 | Moving Average - 27.66 | Min score - 37.56 | Max score - 39.55\n",
      "Episode 41 : Average score - 34.41 | Moving Average - 27.82 | Min score - 24.79 | Max score - 39.48\n",
      "Episode 42 : Average score - 37.47 | Moving Average - 28.05 | Min score - 27.16 | Max score - 39.55\n",
      "Episode 43 : Average score - 37.30 | Moving Average - 28.27 | Min score - 24.17 | Max score - 39.53\n",
      "Episode 44 : Average score - 37.56 | Moving Average - 28.48 | Min score - 15.31 | Max score - 39.51\n",
      "Episode 45 : Average score - 36.93 | Moving Average - 28.67 | Min score - 25.85 | Max score - 39.51\n",
      "Episode 46 : Average score - 35.66 | Moving Average - 28.82 | Min score - 23.03 | Max score - 39.54\n",
      "Episode 47 : Average score - 36.84 | Moving Average - 28.99 | Min score - 31.70 | Max score - 39.49\n",
      "Episode 48 : Average score - 35.69 | Moving Average - 29.13 | Min score - 22.43 | Max score - 39.61\n",
      "Episode 49 : Average score - 33.92 | Moving Average - 29.23 | Min score - 23.42 | Max score - 39.47\n",
      "Episode 50 : Average score - 35.54 | Moving Average - 29.35 | Min score - 24.91 | Max score - 39.24\n",
      "Episode 51 : Average score - 37.19 | Moving Average - 29.51 | Min score - 24.71 | Max score - 39.59\n",
      "Episode 52 : Average score - 38.08 | Moving Average - 29.67 | Min score - 33.14 | Max score - 39.61\n",
      "Episode 53 : Average score - 38.99 | Moving Average - 29.85 | Min score - 35.17 | Max score - 39.64\n",
      "Episode 54 : Average score - 38.56 | Moving Average - 30.01 | Min score - 37.04 | Max score - 39.58\n",
      "Episode 55 : Average score - 38.49 | Moving Average - 30.16 | Min score - 36.06 | Max score - 39.57\n",
      "Episode 56 : Average score - 38.20 | Moving Average - 30.31 | Min score - 35.19 | Max score - 39.59\n",
      "Episode 57 : Average score - 38.51 | Moving Average - 30.45 | Min score - 35.60 | Max score - 39.61\n",
      "Episode 58 : Average score - 38.56 | Moving Average - 30.59 | Min score - 36.19 | Max score - 39.62\n",
      "Episode 59 : Average score - 38.70 | Moving Average - 30.73 | Min score - 35.69 | Max score - 39.61\n",
      "Episode 60 : Average score - 37.77 | Moving Average - 30.84 | Min score - 34.56 | Max score - 39.41\n",
      "Episode 61 : Average score - 37.77 | Moving Average - 30.96 | Min score - 27.34 | Max score - 39.67\n",
      "Episode 62 : Average score - 38.70 | Moving Average - 31.08 | Min score - 36.30 | Max score - 39.56\n",
      "Episode 63 : Average score - 38.19 | Moving Average - 31.20 | Min score - 35.79 | Max score - 39.60\n",
      "Episode 64 : Average score - 37.95 | Moving Average - 31.30 | Min score - 28.53 | Max score - 39.54\n",
      "Episode 65 : Average score - 38.00 | Moving Average - 31.40 | Min score - 34.58 | Max score - 39.27\n",
      "Episode 66 : Average score - 37.36 | Moving Average - 31.49 | Min score - 28.46 | Max score - 39.60\n",
      "Episode 67 : Average score - 38.31 | Moving Average - 31.60 | Min score - 35.66 | Max score - 39.63\n",
      "Episode 68 : Average score - 37.17 | Moving Average - 31.68 | Min score - 33.42 | Max score - 39.66\n",
      "Episode 69 : Average score - 37.35 | Moving Average - 31.76 | Min score - 34.16 | Max score - 39.34\n",
      "Episode 70 : Average score - 38.31 | Moving Average - 31.85 | Min score - 36.06 | Max score - 39.59\n",
      "Episode 71 : Average score - 38.04 | Moving Average - 31.94 | Min score - 32.63 | Max score - 39.67\n",
      "Episode 72 : Average score - 37.72 | Moving Average - 32.02 | Min score - 32.75 | Max score - 39.26\n",
      "Episode 73 : Average score - 38.19 | Moving Average - 32.11 | Min score - 36.01 | Max score - 39.42\n",
      "Episode 74 : Average score - 38.18 | Moving Average - 32.19 | Min score - 34.21 | Max score - 39.37\n",
      "Episode 75 : Average score - 37.39 | Moving Average - 32.26 | Min score - 34.39 | Max score - 39.63\n",
      "Episode 76 : Average score - 38.08 | Moving Average - 32.33 | Min score - 32.28 | Max score - 39.61\n",
      "Episode 77 : Average score - 37.85 | Moving Average - 32.41 | Min score - 32.23 | Max score - 39.55\n",
      "Episode 78 : Average score - 37.96 | Moving Average - 32.48 | Min score - 34.82 | Max score - 39.67\n",
      "Episode 79 : Average score - 38.40 | Moving Average - 32.55 | Min score - 33.48 | Max score - 39.58\n",
      "Episode 80 : Average score - 37.18 | Moving Average - 32.61 | Min score - 31.86 | Max score - 39.58\n",
      "Episode 81 : Average score - 37.98 | Moving Average - 32.68 | Min score - 34.26 | Max score - 39.65\n",
      "Episode 82 : Average score - 38.34 | Moving Average - 32.75 | Min score - 34.34 | Max score - 39.63\n",
      "Episode 83 : Average score - 37.74 | Moving Average - 32.81 | Min score - 35.44 | Max score - 39.24\n",
      "Episode 84 : Average score - 38.32 | Moving Average - 32.87 | Min score - 34.41 | Max score - 39.64\n",
      "Episode 85 : Average score - 38.27 | Moving Average - 32.93 | Min score - 36.02 | Max score - 39.52\n",
      "Episode 86 : Average score - 37.99 | Moving Average - 32.99 | Min score - 35.16 | Max score - 39.58\n",
      "Episode 87 : Average score - 37.48 | Moving Average - 33.04 | Min score - 31.51 | Max score - 39.60\n",
      "Episode 88 : Average score - 37.32 | Moving Average - 33.09 | Min score - 35.19 | Max score - 39.61\n",
      "Episode 89 : Average score - 37.91 | Moving Average - 33.15 | Min score - 34.60 | Max score - 39.46\n",
      "Episode 90 : Average score - 37.13 | Moving Average - 33.19 | Min score - 33.04 | Max score - 39.54\n",
      "Episode 91 : Average score - 38.25 | Moving Average - 33.25 | Min score - 35.59 | Max score - 39.53\n",
      "Episode 92 : Average score - 38.12 | Moving Average - 33.30 | Min score - 35.73 | Max score - 39.63\n",
      "Episode 93 : Average score - 36.61 | Moving Average - 33.34 | Min score - 29.25 | Max score - 39.67\n",
      "Episode 94 : Average score - 36.87 | Moving Average - 33.37 | Min score - 33.32 | Max score - 39.62\n",
      "Episode 95 : Average score - 36.25 | Moving Average - 33.40 | Min score - 31.73 | Max score - 39.39\n",
      "Episode 96 : Average score - 33.66 | Moving Average - 33.41 | Min score - 28.09 | Max score - 37.83\n",
      "Episode 97 : Average score - 29.60 | Moving Average - 33.37 | Min score - 23.02 | Max score - 36.18\n",
      "Episode 98 : Average score - 31.54 | Moving Average - 33.35 | Min score - 23.72 | Max score - 38.39\n",
      "Episode 99 : Average score - 30.13 | Moving Average - 33.32 | Min score - 17.59 | Max score - 38.49\n",
      "Episode 100 : Average score - 30.45 | Moving Average - 33.29 | Min score - 20.40 | Max score - 34.85\n",
      "\n",
      "Environment solved in 0 episodes! Moving Average = 33.29 over last 100 episodes\n"
     ]
    }
   ],
   "source": [
    "from workspace_utils import active_session\n",
    "\n",
    "with active_session():  # used to keep Udacity's workspace awake during training\n",
    "    # train the agent\n",
    "    mean_scores, moving_avgs = ddpg()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f3b9b17ea58>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot the scores\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot()\n",
    "plt.plot(np.arange(len(mean_scores)), mean_scores, label='Average Reward')\n",
    "plt.plot(np.arange(len(mean_scores)), moving_avgs, color='g', label='Moving Average')\n",
    "plt.axhline(30, color=\"r\", linestyle='dashed', label='Target Score')\n",
    "plt.ylabel('Score')\n",
    "plt.xlabel('Episode #')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When finished, you can close the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Possible improvements\n",
    "\n",
    "While DDPG can achieve great performance sometimes, it is frequently brittle with respect to hyperparameters and other kinds of tuning. Implementing other algorithms like Proximal Policy Optimization(PPO) and comparing the obtained results would give us a better picture of what algorithm is best suited for this particular environment.\n",
    "\n",
    "Another thing that would definitely improve our model performance is hyperparameter tuning. We could optimize our model architecture(number of layers, hidden units, etc) and hyperperameters like:\n",
    "\n",
    "- BUFFER_SIZE - replay buffer size\n",
    "- BATCH_SIZE - minibatch size\n",
    "- GAMMA - discount factor\n",
    "- TAU - for soft update of target parameters\n",
    "- LR_ACTOR - learning rate of the actor \n",
    "- LR_CRITIC - learning rate of the critic\n",
    "- WEIGHT_DECAY - L2 weight decay\n",
    "- UPDATE_EVERY - how often to update the network\n",
    "- UPDATE_NUM - number of update passes\n",
    "- EPSILON - starting value of epsilon\n",
    "- EPSILON_DECAY - factor for decreasing epsilon"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
